{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63472d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRATEGIC RECOMMENDATIONS FOR STAKEHOLDERS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      9\u001b[39m recommendations = {\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcritical\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhigh_priority\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmedium_priority\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlow_priority\u001b[39m\u001b[33m'\u001b[39m: []\n\u001b[32m     14\u001b[39m }\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Critical Issues\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mComponent Failure\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mIssueType\u001b[39m\u001b[33m'\u001b[39m].values:\n\u001b[32m     18\u001b[39m     comp_fail_pct = (\u001b[38;5;28mlen\u001b[39m(df[df[\u001b[33m'\u001b[39m\u001b[33mIssueType\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mComponent Failure\u001b[39m\u001b[33m'\u001b[39m]) / \u001b[38;5;28mlen\u001b[39m(df)) * \u001b[32m100\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m comp_fail_pct > \u001b[32m20\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: STRATEGIC RECOMMENDATIONS AND BUSINESS IMPACT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATEGIC RECOMMENDATIONS FOR STAKEHOLDERS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "recommendations = {\n",
    "    'critical': [],\n",
    "    'high_priority': [],\n",
    "    'medium_priority': [],\n",
    "    'low_priority': []\n",
    "}\n",
    "\n",
    "# Critical Issues\n",
    "if 'Component Failure' in df['IssueType'].values:\n",
    "    comp_fail_pct = (len(df[df['IssueType'] == 'Component Failure']) / len(df)) * 100\n",
    "    if comp_fail_pct > 20:\n",
    "        recommendations['critical'].append({\n",
    "            'issue': f'Component Failure Rate: {comp_fail_pct:.1f}%',\n",
    "            'impact': 'High warranty costs and customer satisfaction risk',\n",
    "            'action': '1. Implement rigorous QA testing for radio modules\\n'\n",
    "                     '2. Conduct design review of high-failure components\\n'\n",
    "                     '3. Increase supplier quality standards'\n",
    "        })\n",
    "\n",
    "# Radio Module Issues\n",
    "radio_issues = len(df[df['Components'].apply(lambda x: 'Radio Module' in x if isinstance(x, list) else False)])\n",
    "if radio_issues > 0:\n",
    "    radio_pct = (radio_issues / len(df)) * 100\n",
    "    recommendations['critical'].append({\n",
    "        'issue': f'Radio Module Problems: {radio_pct:.1f}% of cases',\n",
    "        'impact': 'Core infotainment system reliability affects brand reputation',\n",
    "        'action': '1. Audit radio module design for communication protocols\\n'\n",
    "                 '2. Improve ethernet bus reliability testing\\n'\n",
    "                 '3. Enhance firmware stability'\n",
    "    })\n",
    "\n",
    "# Communication Issues\n",
    "comm_issues = len(df[df['IssueType'] == 'Communication/Connectivity Issue'])\n",
    "if comm_issues > 0:\n",
    "    comm_pct = (comm_issues / len(df)) * 100\n",
    "    recommendations['high_priority'].append({\n",
    "        'issue': f'Communication/Connectivity Issues: {comm_pct:.1f}% of cases',\n",
    "        'impact': 'Features dependent on connectivity (OnStar, XM Radio) become unavailable',\n",
    "        'action': '1. Implement redundant communication pathways\\n'\n",
    "                 '2. Improve error handling for network timeouts\\n'\n",
    "                 '3. Add diagnostic tools for connectivity troubleshooting'\n",
    "    })\n",
    "\n",
    "# Intermittent Issues\n",
    "intermittent_count = len(df[df['Failures'].apply(lambda x: 'Intermittent' in x if isinstance(x, list) else False)])\n",
    "if intermittent_count > 0:\n",
    "    recommendations['high_priority'].append({\n",
    "        'issue': f'Intermittent Failures: {(intermittent_count/len(df)*100):.1f}% of cases',\n",
    "        'impact': 'Difficult to diagnose and repair; increases warranty costs',\n",
    "        'action': '1. Implement enhanced logging for intermittent events\\n'\n",
    "                 '2. Improve connector/terminal tension specifications\\n'\n",
    "                 '3. Add predictive diagnostics'\n",
    "    })\n",
    "\n",
    "# Display Issues\n",
    "display_issues = len(df[df['IssueType'] == 'Display/Screen Issue'])\n",
    "if display_issues > 0:\n",
    "    recommendations['high_priority'].append({\n",
    "        'issue': f'Display/Screen Issues: {(display_issues/len(df)*100):.1f}%',\n",
    "        'impact': 'User experience severely impacted; critical safety concerns for backup camera',\n",
    "        'action': '1. Review display driver compatibility\\n'\n",
    "                 '2. Enhance thermal management for LCD\\n'\n",
    "                 '3. Redesign connector with better secure locking'\n",
    "    })\n",
    "\n",
    "# Software/Update Issues\n",
    "software_issues = len(df[df['IssueType'] == 'Software/Update Issue'])\n",
    "if software_issues > 0:\n",
    "    recommendations['high_priority'].append({\n",
    "        'issue': f'Software/Update Issues: {(software_issues/len(df)*100):.1f}%',\n",
    "        'impact': 'Power users unable to access latest features; service center burden',\n",
    "        'action': '1. Improve OTA update robustness\\n'\n",
    "                 '2. Add rollback capability for failed updates\\n'\n",
    "                 '3. Implement more thorough pre-release testing'\n",
    "    })\n",
    "\n",
    "# Print recommendations\n",
    "print(\"ðŸ”´ CRITICAL ACTIONS REQUIRED:\\n\")\n",
    "for rec in recommendations['critical']:\n",
    "    print(f\"  Issue: {rec['issue']}\")\n",
    "    print(f\"  Impact: {rec['impact']}\")\n",
    "    print(f\"  Actions:\\n    {rec['action'].replace(chr(10), chr(10) + '    ')}\\n\")\n",
    "\n",
    "print(\"\\nðŸŸ¡ HIGH PRIORITY IMPROVEMENTS:\\n\")\n",
    "for rec in recommendations['high_priority']:\n",
    "    print(f\"  Issue: {rec['issue']}\")\n",
    "    print(f\"  Impact: {rec['impact']}\")\n",
    "    print(f\"  Actions:\\n    {rec['action'].replace(chr(10), chr(10) + '    ')}\\n\")\n",
    "\n",
    "# Expected Business Impact\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPECTED BUSINESS IMPACT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "total_warranty_events = len(df)\n",
    "avg_cost_per_repair = 150  # Estimated\n",
    "total_current_cost = total_warranty_events * avg_cost_per_repair\n",
    "\n",
    "# Project improvements\n",
    "improvement_scenarios = {\n",
    "    'Reduce Component Failures by 30%': {\n",
    "        'reduction': 0.30,\n",
    "        'issue': 'Component Failure',\n",
    "        'metric': 'warranty costs'\n",
    "    },\n",
    "    'Reduce Communication Issues by 40%': {\n",
    "        'reduction': 0.40,\n",
    "        'issue': 'Communication/Connectivity Issue',\n",
    "        'metric': 'service center tickets'\n",
    "    },\n",
    "    'Reduce Intermittent Issues by 50%': {\n",
    "        'reduction': 0.50,\n",
    "        'issue': 'Intermittent Issue',\n",
    "        'metric': 'repeat visits'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Potential Cost Savings (Annual Projection):\\n\")\n",
    "total_savings = 0\n",
    "for scenario, params in improvement_scenarios.items():\n",
    "    issue_cases = len(df[df['IssueType'] == params['issue']])\n",
    "    potential_savings = issue_cases * params['reduction'] * avg_cost_per_repair\n",
    "    total_savings += potential_savings\n",
    "    print(f\"  {scenario}:\")\n",
    "    print(f\"    Cases Affected: {issue_cases}\")\n",
    "    print(f\"    Potential Savings: ${potential_savings:,.0f}\\n\")\n",
    "\n",
    "print(f\"  TOTAL POTENTIAL ANNUAL SAVINGS: ${total_savings:,.0f}\")\n",
    "print(f\"  ROI Target: Invest in improvements to realize {(total_savings/total_current_cost*100):.1f}% cost reduction\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ TASK 2 ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad404a24",
   "metadata": {},
   "source": [
    "## Section 6: Strategic Recommendations and Business Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: ROOT CAUSE ANALYSIS AND INSIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROOT CAUSE ANALYSIS AND INSIGHTS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Identify key failure patterns\n",
    "print(\"--- RECURRING FAILURE PATTERNS ---\")\n",
    "\n",
    "# Component-Failure correlation\n",
    "component_failure_matrix = {}\n",
    "for idx, row in df.iterrows():\n",
    "    for component in row['Components']:\n",
    "        for failure in row['Failures']:\n",
    "            key = f\"{component}|{failure}\"\n",
    "            component_failure_matrix[key] = component_failure_matrix.get(key, 0) + 1\n",
    "\n",
    "sorted_patterns = sorted(component_failure_matrix.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop Component-Failure Combinations:\")\n",
    "for idx, (pattern, count) in enumerate(sorted_patterns[:15], 1):\n",
    "    comp, fail = pattern.split('|')\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"{idx:2d}. {comp:25s} â†’ {fail:30s} : {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Issue Type Analysis\n",
    "print(\"\\n--- ISSUE TYPE ROOT CAUSE ANALYSIS ---\")\n",
    "for issue_type in df['IssueType'].unique():\n",
    "    if issue_type == 'Unknown':\n",
    "        continue\n",
    "    issue_data = df[df['IssueType'] == issue_type]\n",
    "    print(f\"\\n{issue_type} ({len(issue_data)} cases):\")\n",
    "    \n",
    "    # Top components\n",
    "    all_comps = [c for comps in issue_data['Components'] for c in comps]\n",
    "    if all_comps:\n",
    "        print(f\"  Primary Components: {Counter(all_comps).most_common(3)}\")\n",
    "    \n",
    "    # Top solutions  \n",
    "    all_sols = [s for sols in issue_data['Solutions'] for s in sols]\n",
    "    if all_sols:\n",
    "        print(f\"  Common Solutions: {Counter(all_sols).most_common(2)}\")\n",
    "    \n",
    "    # Recurrence rate\n",
    "    recurring = (len(issue_data[issue_data.duplicated(subset=['MAKE', 'MODEL'], keep=False)]) / len(issue_data)) * 100\n",
    "    print(f\"  Recurrence Rate: {recurring:.1f}%\")\n",
    "\n",
    "# Failure severity assessment\n",
    "print(\"\\n--- FAILURE SEVERITY ASSESSMENT ---\")\n",
    "severity_factors = {}\n",
    "for issue_type in df['IssueType'].unique():\n",
    "    issue_data = df[df['IssueType'] == issue_type]\n",
    "    severity_factors[issue_type] = {\n",
    "        'frequency': len(issue_data) / len(df),\n",
    "        'avg_resolution_complexity': (len(issue_data['Solutions'].apply(len)).sum() / len(issue_data)) if len(issue_data) > 0 else 0\n",
    "    }\n",
    "\n",
    "severity_df = pd.DataFrame(severity_factors).T\n",
    "severity_df['severity_score'] = severity_df['frequency'] * severity_df['avg_resolution_complexity'] * 100\n",
    "severity_df = severity_df.sort_values('severity_score', ascending=False)\n",
    "\n",
    "print(\"\\nSeverity Ranking (by Frequency Ã— Complexity):\")\n",
    "for issue, score in severity_df['severity_score'].items():\n",
    "    print(f\"  {issue:35s}: {score:6.2f}\")\n",
    "\n",
    "# Export extracted data with tags\n",
    "print(\"\\n--- EXPORTING ANALYSIS RESULTS ---\")\n",
    "export_df = df[[\n",
    "    'Event id', 'Opened date', 'MAKE', 'MODEL', 'COMPLAINT_CD_DESC', \n",
    "    'CAUSAL_CD_DESC', 'IssueType', 'Components', 'Failures', 'Solutions', 'Cluster'\n",
    "]].copy()\n",
    "\n",
    "# Convert list columns to string for CSV\n",
    "export_df['Components'] = export_df['Components'].apply(lambda x: '|'.join(x) if isinstance(x, list) else '')\n",
    "export_df['Failures'] = export_df['Failures'].apply(lambda x: '|'.join(x) if isinstance(x, list) else '')\n",
    "export_df['Solutions'] = export_df['Solutions'].apply(lambda x: '|'.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "csv_path = 'c:/Users/lavan/OneDrive/Desktop/hanu.ai/Task2_Analyzed_Data_with_Tags.csv'\n",
    "export_df.to_csv(csv_path, index=False)\n",
    "logger.info(f\"âœ“ Analysis results exported to: {csv_path}\")\n",
    "\n",
    "# Generate insights summary\n",
    "insights_summary = {\n",
    "    'total_issues': len(df),\n",
    "    'issue_type_distribution': df['IssueType'].value_counts().to_dict(),\n",
    "    'top_components': Counter([c for comps in df['Components'] for c in comps]).most_common(10),\n",
    "    'top_failures': Counter([f for fails in df['Failures'] for f in fails]).most_common(10),\n",
    "    'top_solutions': Counter([s for sols in df['Solutions'] for s in sols]).most_common(10),\n",
    "    'severity_ranking': severity_df['severity_score'].to_dict(),\n",
    "    'vehicle_complaint_distribution': df.groupby('MODEL')['IssueType'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "json_path = 'c:/Users/lavan/OneDrive/Desktop/hanu.ai/task2_insights.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(insights_summary, f, indent=2, default=str)\n",
    "logger.info(f\"âœ“ Insights summary saved to: {json_path}\")\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6d8ca",
   "metadata": {},
   "source": [
    "## Section 5: Root Cause Analysis and Insights\n",
    "\n",
    "Identify root causes, failure patterns, and actionable recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: CLUSTERING AND TOPIC MODELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING AND TOPIC MODELING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Prepare text for clustering\n",
    "logger.info(\"Preparing text for clustering...\")\n",
    "combined_text = df['CAUSAL_VERBATIM'].fillna('') + ' ' + df['CORRECTION_VERBATIM'].fillna('')\n",
    "combined_text = combined_text.fillna('').str.lower()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.8, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_text)\n",
    "logger.info(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# K-Means Clustering (optimal k using elbow method)\n",
    "logger.info(\"Performing K-Means clustering...\")\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(tfidf_matrix, kmeans.labels_))\n",
    "\n",
    "# Use k=4 (good balance)\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(tfidf_matrix)\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"Cluster Distribution (k={optimal_k}):\")\n",
    "print(df['Cluster'].value_counts().sort_index())\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\n--- CLUSTER CHARACTERIZATION ---\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df[df['Cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_data)} issues):\")\n",
    "    print(f\"  Primary Issue Types: {cluster_data['IssueType'].value_counts().head(3).to_dict()}\")\n",
    "    print(f\"  Top Components: {dict(pd.Series([c for comps in cluster_data['Components'] for c in comps]).value_counts().head(3))}\")\n",
    "    print(f\"  Most Common Failures: {dict(pd.Series([f for fails in cluster_data['Failures'] for f in fails]).value_counts().head(2))}\")\n",
    "\n",
    "# Topic Modeling (LDA)\n",
    "logger.info(\"Performing Topic Modeling (LDA)...\")\n",
    "\n",
    "# Prepare corpus\n",
    "def preprocess_for_lda(text):\n",
    "    \"\"\"Simple preprocessing for LDA\"\"\"\n",
    "    text = str(text).lower()\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', text)  # Keep 3+ char words\n",
    "    return [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "corpus_texts = combined_text.apply(preprocess_for_lda)\n",
    "dictionary = corpora.Dictionary(corpus_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus_texts]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=10)\n",
    "\n",
    "print(\"\\n--- LATENT DIRICHLET ALLOCATION TOPICS ---\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Visualize clustering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "ax1 = axes[0]\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "ax1.set_ylabel('Inertia', fontsize=11)\n",
    "ax1.set_title('K-Means Elbow Curve', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cluster distribution\n",
    "ax2 = axes[1]\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_counts)))\n",
    "ax2.bar(['Cluster ' + str(i) for i in cluster_counts.index], cluster_counts.values, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Issues', fontsize=11)\n",
    "ax2.set_title(f'Distribution of Issues across {optimal_k} Clusters', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('c:/Users/lavan/OneDrive/Desktop/hanu.ai/task2_clustering.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Clustering analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd7a68",
   "metadata": {},
   "source": [
    "## Section 4: Clustering and Topic Modeling\n",
    "\n",
    "Group failure modes and customer complaints into categories using K-Means clustering and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54653e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: TEXT MINING AND ENTITY EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT MINING AND ENTITY EXTRACTION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def extract_entities_from_text(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract entities and tags from warranty claim text.\n",
    "    Identifies: Components, Failure Types, Solutions, Symptoms\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {'Components': [], 'Failures': [], 'Solutions': [], 'Symptoms': []}\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    entities = {'Components': [], 'Failures': [], 'Solutions': [], 'Symptoms': []}\n",
    "    \n",
    "    # Component extraction patterns\n",
    "    component_keywords = {\n",
    "        'Radio Module': ['radio', 'radio module', 'radio assembly', 'radio control'],\n",
    "        'Display/Screen': ['display', 'screen', 'lcd', 'touch screen', 'infotainment'],\n",
    "        'Antenna': ['antenna', 'coaxial', 'coax', 'satellite antenna'],\n",
    "        'Audio System': ['audio', 'speaker', 'amplifier', 'amp', 'sound'],\n",
    "        'Communication': ['ethernet', 'communication', 'onstar', 'bluetooth', 'wifi'],\n",
    "        'SD Card': ['sd card', 'card reader', 'receptacle'],\n",
    "        'USB': ['usb', 'usb port'],\n",
    "        'HVAC': ['hvac', 'climate', 'temperature control'],\n",
    "        'Camera': ['camera', 'backup camera', 'rear view'],\n",
    "        'Battery': ['battery', 'power', 'voltage']\n",
    "    }\n",
    "    \n",
    "    for component, keywords in component_keywords.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            entities['Components'].append(component)\n",
    "    \n",
    "    # Failure type extraction\n",
    "    failure_keywords = {\n",
    "        'No Communication': ['no communication', 'no comm', '  loss of communication', 'not responding'],\n",
    "        'Black Screen': ['black screen', 'screen goes blank', 'blank screen', 'screen dark'],\n",
    "        'No Audio': ['no audio', 'no sound', 'audio cut out', 'mute'],\n",
    "        'Intermittent': ['intermittent', 'cuts out', 'randomly', 'sometimes', 'periodically'],\n",
    "        'Freezing': ['freeze', 'freezing', 'frozen', 'locked'],\n",
    "        'Internal Fault': ['internal fault', 'internal failure', 'internal issue', 'malfunction'],\n",
    "        'Inoperative': ['inoperative', 'inop', 'not working', 'not functioning'],\n",
    "        'Demo Mode': ['demo mode', 'demonstration mode'],\n",
    "        'Update Failure': ['update failed', 'programming failed', 'software failed'],\n",
    "        'Error Message': ['error', 'error code', 'error message']\n",
    "    }\n",
    "    \n",
    "    for failure, keywords in failure_keywords.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            entities['Failures'].append(failure)\n",
    "    \n",
    "    # Solution extraction\n",
    "    solution_keywords = {\n",
    "        'Radio Replacement': ['replace radio', 'replaced radio', 'radio replacement'],\n",
    "        'Programming': ['programming', 'reprogramming', 'programmed', 'sps', 'usb program'],\n",
    "        'Software Update': ['update', 'software update', 'firmware update', 'patch'],\n",
    "        'Component Replacement': ['replace', 'replaced', 'replacement', 'new component'],\n",
    "        'Wiring/Connector Fix': ['connector', 'wiring', 'pin', 'terminal tension', 'depinned'],\n",
    "        'Reset': ['reset', 'global reset', 'factory reset', 'restart'],\n",
    "        'TAC Case/Support': ['tac', 'techline', 'support case', 'engineering']\n",
    "    }\n",
    "    \n",
    "    for solution, keywords in solution_keywords.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            entities['Solutions'].append(solution)\n",
    "    \n",
    "    # Symptom extraction\n",
    "    symptom_keywords = {\n",
    "        'Not Powering On': ['not power', 'will not turn on', 'wont turn on', 'power on'],\n",
    "        'Connectivity Loss': ['lost connection', 'loss of signal', 'signal lost'],\n",
    "        'Slow Performance': ['slow', 'sluggish', 'lag', 'delayed'],\n",
    "        'Crashing/Reboot': ['crash', 'reboot', 'restart', 'reset to factory'],\n",
    "        'Partial Functionality': ['half the radio', 'partially working', 'some functions']\n",
    "    }\n",
    "    \n",
    "    for symptom, keywords in symptom_keywords.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            entities['Symptoms'].append(symptom)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply entity extraction to key text columns\n",
    "logger.info(\"Extracting entities from text columns...\")\n",
    "text_columns = ['CAUSAL_VERBATIM', 'CORRECTION_VERBATIM', 'CUSTOMER_VERBATIM']\n",
    "\n",
    "extracted_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    combined_text = ' '.join([str(row.get(col, '')) for col in text_columns if pd.notna(row.get(col))])\n",
    "    entities = extract_entities_from_text(combined_text)\n",
    "    entities['EventId'] = row['Event id']\n",
    "    extracted_data.append(entities)\n",
    "\n",
    "entities_df = pd.DataFrame(extracted_data)\n",
    "df = df.merge(entities_df, left_on='Event id', right_on='EventId', how='left')\n",
    "\n",
    "logger.info(f\"âœ“ Extracted entities for {len(entities_df)} events\")\n",
    "\n",
    "# Analyze extracted entities\n",
    "print(\"--- EXTRACTED ENTITIES SUMMARY ---\")\n",
    "all_components = []\n",
    "all_failures = []\n",
    "all_solutions = []\n",
    "all_symptoms = []\n",
    "\n",
    "for _, row in entities_df.iterrows():\n",
    "    all_components.extend(row['Components'])\n",
    "    all_failures.extend(row['Failures'])\n",
    "    all_solutions.extend(row['Solutions'])\n",
    "    all_symptoms.extend(row['Symptoms'])\n",
    "\n",
    "print(\"\\nTop Components:\")\n",
    "component_counts = pd.Series(all_components).value_counts()\n",
    "for comp, count in component_counts.head(10).items():\n",
    "    print(f\"  {comp:30s}: {count:3d} ({count/len(entities_df)*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\nTop Failures:\")\n",
    "failure_counts = pd.Series(all_failures).value_counts()\n",
    "for fail, count in failure_counts.head(10).items():\n",
    "    print(f\"  {fail:30s}: {count:3d} ({count/len(entities_df)*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\nTop Solutions:\")\n",
    "solution_counts = pd.Series(all_solutions).value_counts()\n",
    "for sol, count in solution_counts.head(10).items():\n",
    "    print(f\"  {sol:30s}: {count:3d} ({count/len(entities_df)*100:5.1f}%)\")\n",
    "\n",
    "# Create Issue Type Categories\n",
    "def categorize_issue_type(failures: list, components: list) -> str:\n",
    "    \"\"\"Categorize issue type based on extracted entities\"\"\"\n",
    "    if not failures and not components:\n",
    "        return 'Unknown'\n",
    "    \n",
    "    failure_str = ' '.join(failures).lower()\n",
    "    component_str = ' '.join(components).lower()\n",
    "    \n",
    "    if 'internal fault' in failure_str or 'malfunction' in failure_str:\n",
    "        return 'Component Failure'\n",
    "    elif 'no communication' in failure_str or 'not responding' in failure_str:\n",
    "        return 'Communication/Connectivity Issue'\n",
    "    elif 'black screen' in failure_str or 'screen' in failure_str:\n",
    "        return 'Display/Screen Issue'\n",
    "    elif 'no audio' in failure_str or 'no sound' in failure_str:\n",
    "        return 'Audio Issue'\n",
    "    elif 'intermittent' in failure_str:\n",
    "        return 'Intermittent Issue'\n",
    "    elif 'update failed' in failure_str:\n",
    "        return 'Software/Update Issue'\n",
    "    elif 'wiring' in component_str or 'connector' in component_str:\n",
    "        return 'Electrical/Wiring Issue'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['IssueType'] = df.apply(lambda row: categorize_issue_type(row['Failures'], row['Components']), axis=1)\n",
    "\n",
    "print(\"\\n--- ISSUE TYPE DISTRIBUTION ---\")\n",
    "issue_dist = df['IssueType'].value_counts()\n",
    "for issue, count in issue_dist.items():\n",
    "    print(f\"  {issue:35s}: {count:3d} ({count/len(df)*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ee778",
   "metadata": {},
   "source": [
    "## Section 3: Text Mining and Entity Extraction\n",
    "\n",
    "Extract meaningful entities, tags, and issue types from free text fields (CAUSAL_VERBATIM, CORRECTION_VERBATIM, CUSTOMER_VERBATIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EDA Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Top Complaint Categories\n",
    "ax1 = axes[0, 0]\n",
    "complaint_dist.head(10).plot(kind='barh', ax=ax1, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Count', fontsize=11)\n",
    "ax1.set_title('Top 10 Complaint Categories', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Top Causal Categories\n",
    "ax2 = axes[0, 1]\n",
    "causal_dist.head(10).plot(kind='barh', ax=ax2, color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('Count', fontsize=11)\n",
    "ax2.set_title('Top 10 Causal Categories', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Vehicle Distribution\n",
    "ax3 = axes[1, 0]\n",
    "vehicle_dist.head(8).plot(kind='bar', ax=ax3, color='teal', edgecolor='black')\n",
    "ax3.set_ylabel('Count', fontsize=11)\n",
    "ax3.set_title('Top 8 Vehicle Models', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: Service Plant Distribution\n",
    "ax4 = axes[1, 1]\n",
    "plant_dist.plot(kind='bar', ax=ax4, color='mediumseagreen', edgecolor='black')\n",
    "ax4.set_ylabel('Count', fontsize=11)\n",
    "ax4.set_title('Service Plant Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('c:/Users/lavan/OneDrive/Desktop/hanu.ai/task2_eda_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ EDA visualizations saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a611cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. Data Overview\n",
    "print(\"--- DATASET DIMENSIONS & STRUCTURE ---\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Data Types:\\n{df.dtypes}\")\n",
    "\n",
    "# 2. Missing Values Analysis\n",
    "print(\"\\n--- MISSING VALUE ANALYSIS ---\")\n",
    "missing_stats = df.isnull().sum()\n",
    "missing_pct = (missing_stats / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "print(missing_df)\n",
    "\n",
    "# Identify free text columns (>50% non-null, non-numeric)\n",
    "free_text_cols = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' and df[col].str.len().mean() > 50:\n",
    "        free_text_cols.append(col)\n",
    "        \n",
    "logger.info(f\"\\nFree Text Columns Identified: {free_text_cols}\")\n",
    "\n",
    "# 3. Key Statistics\n",
    "print(\"\\n--- KEY STATISTICS ---\")\n",
    "print(f\"Total Events: {len(df)}\")\n",
    "print(f\"Date Range: {df['Opened date'].min()} to {df['Opened date'].max()}\")\n",
    "print(f\"Unique Makes: {df['MAKE'].nunique()}\")\n",
    "print(f\"Unique Models: {df['MODEL'].nunique()}\")\n",
    "print(f\"Unique Complaint Categories: {df['COMPLAINT_CD_DESC'].nunique()}\")\n",
    "print(f\"Unique Causal Categories: {df['CAUSAL_CD_DESC'].nunique()}\")\n",
    "\n",
    "# 4. Complaint Type Distribution\n",
    "print(\"\\n--- TOP COMPLAINT CATEGORIES ---\")\n",
    "complaint_dist = df['COMPLAINT_CD_DESC'].value_counts().head(15)\n",
    "for idx, (cat, count) in enumerate(complaint_dist.items(), 1):\n",
    "    print(f\"{idx:2d}. {cat:50s} : {count:3d} ({count/len(df)*100:5.1f}%)\")\n",
    "\n",
    "# 5. Causal Category Distribution\n",
    "print(\"\\n--- TOP CAUSAL CATEGORIES ---\")\n",
    "causal_dist = df['CAUSAL_CD_DESC'].value_counts().head(10)\n",
    "for idx, (cat, count) in enumerate(causal_dist.items(), 1):\n",
    "    print(f\"{idx:2d}. {cat:50s} : {count:3d} ({count/len(df)*100:5.1f}%)\")\n",
    "\n",
    "# 6. Vehicle Distribution\n",
    "print(\"\\n--- TOP VEHICLE MODELS ---\")\n",
    "vehicle_dist = df['MODEL'].value_counts().head(10)\n",
    "for idx, (model, count) in enumerate(vehicle_dist.items(), 1):\n",
    "    print(f\"{idx:2d}. {model:30s} : {count:3d} ({count/len(df)*100:5.1f}%)\")\n",
    "\n",
    "# 7. Plant / Location Analysis\n",
    "print(\"\\n--- SERVICE PLANTS ---\")\n",
    "plant_dist = df['PLANT'].value_counts()\n",
    "for idx, (plant, count) in enumerate(plant_dist.items(), 1):\n",
    "    print(f\"{idx:2d}. {plant:30s} : {count:3d} ({count/len(df)*100:5.1f}%)\")\n",
    "\n",
    "# 8. Text Length Analysis\n",
    "print(\"\\n--- TEXT LENGTH STATISTICS (for free text columns) ---\")\n",
    "for col in free_text_cols[:3]:  # First 3 text columns\n",
    "    if col in df.columns:\n",
    "        lengths = df[col].fillna('').str.len()\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {lengths.mean():.0f} chars\")\n",
    "        print(f\"  Median: {lengths.median():.0f} chars\")\n",
    "        print(f\"  Max: {lengths.max():.0f} chars\")\n",
    "\n",
    "# 9. Duplicates Check\n",
    "print(\"\\n--- DUPLICATES CHECK ---\")\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "logger.info(f\"Duplicate complete rows: {duplicate_rows}\")\n",
    "\n",
    "# 10. Data Completeness Score\n",
    "print(\"\\n--- DATA COMPLETENESS ---\")\n",
    "completeness = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "print(f\"Overall Completeness: {completeness:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f1d13",
   "metadata": {},
   "source": [
    "## Section 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e093f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LdaModel\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: SETUP AND DATA LOADING\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "# NLP Libraries (optional ones are handled gracefully)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# Optional NLP packages (spacy, textblob) are not required for core analysis\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    TEXTBLOB_AVAILABLE = True\n",
    "except Exception:\n",
    "    TEXTBLOB_AVAILABLE = False\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# gensim is optional for advanced LDA; handle gracefully if missing\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "    from gensim.models import LdaModel\n",
    "    GENSIM_AVAILABLE = True\n",
    "except Exception:\n",
    "    GENSIM_AVAILABLE = False\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.warning(\"gensim not available - LDA topics will be skipped or use sklearn NMF instead\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download NLTK data (best-effort; continue if network restricted)\n",
    "nltk_resources = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'vader_lexicon']\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        nltk.data.find(resource if resource != 'vader_lexicon' else 'sentiment/vader_lexicon')\n",
    "    except Exception:\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "        except Exception:\n",
    "            logger.warning(f\"Could not download NLTK resource: {resource} - continuing without it\")\n",
    "\n",
    "print(\"âœ“ Libraries loaded and NLTK data configured (optional packages may be unavailable)\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING WARRANTY CLAIMS DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dataset_path = 'c:/Users/lavan/OneDrive/Desktop/hanu.ai/Task2_Dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "logger.info(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "logger.info(f\"Columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e8b85d",
   "metadata": {},
   "source": [
    "# Advanced EDA & Text Mining - Automotive Warranty Claims Dataset\n",
    "\n",
    "## Task Objective\n",
    "Perform comprehensive exploratory data analysis, text mining, and actionable insight generation on automotive warranty claim data to:\n",
    "- Understand data types, volume, and characteristics\n",
    "- Identify and resolve data quality issues\n",
    "- Extract meaningful entities from free text fields\n",
    "- Categorize issues into complaint types\n",
    "- Apply clustering/topic modeling to group failure modes\n",
    "- Identify root causes for recurring failures\n",
    "- Generate actionable recommendations for product improvement\n",
    "\n",
    "### Dataset Overview\n",
    "- **Domain**: Automotive Warranty Claims\n",
    "- **Key Focus**: Radio/Infotainment System Failures\n",
    "- **Variables**: Event details, customer complaints, diagnoses, corrections, vehicle info\n",
    "- **Expected Outcomes**: Failure pattern identification, root cause analysis, improvement recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
